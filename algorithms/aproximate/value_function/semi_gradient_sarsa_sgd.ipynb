{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08991c7e-b8d7-4288-9fc9-c09ed9157210",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2343803-cfc4-4306-837c-b75bc40fc904",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    ipympl \\\n",
    "    ipywidgets \\\n",
    "    moviepy \\\n",
    "    scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "50124529-40dd-4da4-bc19-fbe57dd3b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import sys\n",
    "from typing import *\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "State = Tuple[float, ...]\n",
    "\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "\n",
    "def println(text: str):\n",
    "    sys.stdout.write(\"\\r\" + text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1358f44b-b861-4180-bbcd-70bc0b3c9ae3",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ae5a00d0-94ff-41a4-86c8-45ddeea72df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# featurizer\n",
    "num_tilings = 8\n",
    "tiling_size = 1024\n",
    "# SARSA\n",
    "num_episodes = 3000\n",
    "max_ep_steps = 200\n",
    "discount = 0.95\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50188c7e-52ce-45e8-aaa0-174ef4638ce1",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d3fe651e-db32-4d73-902e-75a4c42c2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# env\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "obs = env.observation_space\n",
    "boundaries = list(zip(obs.low, obs.high))\n",
    "\n",
    "\n",
    "def run_until_random_success():\n",
    "    done = 0\n",
    "    ep = -1\n",
    "    while True:\n",
    "        ep += 1\n",
    "        state, _ = env.reset()\n",
    "        for t in itertools.count():\n",
    "            a = np.random.choice(range(env.action_space.n))\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(a)\n",
    "\n",
    "            if terminated:\n",
    "                print(f\"ep:{ep} - steps:{t}\")\n",
    "                done += 1\n",
    "                break\n",
    "\n",
    "        if done > 5:\n",
    "            break\n",
    "\n",
    "\n",
    "def generate_states(skip_dims: Optional[List[int]] = None):\n",
    "    # Generate X, Y coordinates\n",
    "    ranges = []\n",
    "    box = env.observation_space\n",
    "    if box.bounded_above.any() and box.bounded_below.any():\n",
    "        for i, (l, h) in enumerate(zip(box.low, box.high)):\n",
    "            if skip_dims and i in skip_dims:\n",
    "                continue\n",
    "            step_size = (h - l) / num_tilings / 10\n",
    "            ranges.append(np.arange(l, h, step_size))\n",
    "\n",
    "    return ranges\n",
    "\n",
    "\n",
    "state_ranges = generate_states()\n",
    "states = list(product(*state_ranges))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616ab2b1-2750-4482-b513-926dca093950",
   "metadata": {},
   "source": [
    "## Featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1be49e48-7c3d-453b-b917-6fcf7d2bf5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31297/1945568910.py:13: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  [num_tilings * s / (b[1] - b[0]) for s, b in zip(state, boundaries)],\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6400, 1024)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "from helpers.features.tile_coding import IHT, tiles\n",
    "\n",
    "iht = IHT(tiling_size)\n",
    "\n",
    "\n",
    "def tile_featurize(states: Union[State, List[State]]) -> np.array:\n",
    "    def _feat(state):\n",
    "        indices = tiles(\n",
    "            iht,\n",
    "            num_tilings,\n",
    "            [num_tilings * s / (b[1] - b[0]) for s, b in zip(state, boundaries)],\n",
    "        )\n",
    "        # 1-hot encoding\n",
    "        x = np.zeros(tiling_size)\n",
    "        x[indices] = 1\n",
    "        return x\n",
    "\n",
    "    if isinstance(states, list):\n",
    "        return np.array([_feat(s) for s in states])\n",
    "\n",
    "    return _feat(states)\n",
    "\n",
    "\n",
    "tile_featurize(states).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ceb4b-2217-4952-be4a-9876dd41ce2c",
   "metadata": {},
   "source": [
    "## Value Function Aproximator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "08cb8866-c8bb-4bc3-955a-f7d336035d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "\n",
    "class Estimator:\n",
    "    \"\"\"\n",
    "    Value Function approximator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # We create a separate model for each action in the environment's\n",
    "        # action space. Alternatively we could somehow encode the action\n",
    "        # into the features, but this way it's easier to code up.\n",
    "        self.models = []\n",
    "        s, _ = env.reset()\n",
    "        for _ in range(env.action_space.n):\n",
    "            model = SGDRegressor(learning_rate=\"constant\")\n",
    "            # We need to call partial_fit once to initialize the model\n",
    "            # or we get a NotFittedError when trying to make a prediction\n",
    "            # This is quite hacky.\n",
    "            model.partial_fit([tile_featurize(s)], [0])\n",
    "            self.models.append(model)\n",
    "\n",
    "    def __call__(self, s, a=None):\n",
    "        \"\"\"\n",
    "        Makes value function predictions.\n",
    "\n",
    "        Args:\n",
    "            s: state to make a prediction for\n",
    "            a: (Optional) action to make a prediction for\n",
    "\n",
    "        Returns\n",
    "            If an action a is given this returns a single number as the prediction.\n",
    "            If no action is given this returns a vector or predictions for all actions\n",
    "            in the environment where pred[i] is the prediction for action i.\n",
    "\n",
    "        \"\"\"\n",
    "        features = tile_featurize(s)\n",
    "        if not a:\n",
    "            return np.array([m.predict([features])[0] for m in self.models])\n",
    "        else:\n",
    "            return self.models[a].predict([features])[0]\n",
    "\n",
    "    def update(self, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator parameters for a given state and action towards\n",
    "        the target y.\n",
    "        \"\"\"\n",
    "        features = tile_featurize(s)\n",
    "        self.models[a].partial_fit([features], [y])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb79301f-c671-4133-841f-9b39f455466e",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dab75fd3-7a18-4456-89b5-f357666e5ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_COLORS = [(1, 0, 0, 0.3), (0.3, 0.3, 0.3, 0.3), (0, 0, 1, 0.3)]\n",
    "\n",
    "\n",
    "def plot_q_map(state_ranges, q):\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    x, y = state_ranges\n",
    "    m_shape = (x.shape[0], y.shape[0])\n",
    "    plt.imshow(\n",
    "        q.argmax(axis=-1).reshape(m_shape),\n",
    "    )\n",
    "    loc = [l * 4 for l in range(m_shape[0] // 4)]\n",
    "    plt.xticks(loc, [f\"{x[l]:.2f}\" for l in loc], rotation=45)\n",
    "    plt.yticks(loc, [f\"{y[l]:.2f}\" for l in loc])\n",
    "\n",
    "\n",
    "def plot_q_surface(title=\"Q-value function\"):\n",
    "    x, y = np.meshgrid(*state_ranges)\n",
    "\n",
    "    # Max over actions as the State value\n",
    "    q = np.array([Q(s) for s in states])\n",
    "    z = q.max(axis=-1).reshape(x.shape)\n",
    "\n",
    "    # Colorize the plot based on the chose action\n",
    "    colors = np.array([ACTION_COLORS[i] for i in q.argmax(axis=-1)]).reshape(\n",
    "        (*x.shape, 4)\n",
    "    )\n",
    "\n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    ax.set_title(title)\n",
    "    ax.plot_surface(x, y, z, cmap=\"viridis\", edgecolor=\"green\", facecolors=colors)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80b8140-462f-4eb4-9bf7-a7b955318736",
   "metadata": {},
   "source": [
    "## Semi-gradient SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d563bd9-c6b8-4536-9909-8d2587902b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import random\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def policy(state: State) -> int:\n",
    "    \"\"\"Run the current policy. In this case e-greedy with constant epsilon\n",
    "\n",
    "    Args:\n",
    "        state (int): agent state\n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return np.random.choice(range(env.action_space.n))\n",
    "\n",
    "    return np.argmax(Q(state))\n",
    "\n",
    "\n",
    "def observe(n_step_buffer: List[Tuple[Any, ...]]):\n",
    "    n = len(n_step_buffer) - 1\n",
    "    state, action, reward, _, _ = n_step_buffer[0]\n",
    "    next_state, next_action, _, _, _ = n_step_buffer[-1]\n",
    "    # Compute G_t\n",
    "    # n-step SARSA update:\n",
    "    # If n == 1 (i.e.: 1-step SARSA update), then:\n",
    "    # td_target = reward + discount * Q(next_state)[next_action]\n",
    "    g = (\n",
    "        sum(discount**i * r for i, (_, _, r, _, _) in enumerate(n_step_buffer[:-1]))\n",
    "        + discount**n * Q(next_state)[next_action]\n",
    "    )\n",
    "    Q.update(state, action, g)\n",
    "\n",
    "\n",
    "def learn(n: int = 4) -> Dict[str, Any]:\n",
    "    \"\"\"Implements the On-policy TD Control algorithm 'n-step Semi Gradient SARSA'\n",
    "\n",
    "    Args:\n",
    "        num_episodes (int): max number of episodes\n",
    "        max_ep_steps (int): max number of steps per episode\n",
    "        discount (float): discount factor (gamma)\n",
    "        epsilon (float): probability of taking a random action (epsilon-greedy)\n",
    "        n (int): n-step return update target\n",
    "    \"\"\"\n",
    "    print(\"Start learning\")\n",
    "    stats = {\n",
    "        \"ep_length\": np.zeros(num_episodes),\n",
    "        \"ep_rewards\": np.zeros(num_episodes),\n",
    "        \"ep_loss\": np.zeros(num_episodes),\n",
    "    }\n",
    "\n",
    "    episode_iter = tqdm(range(num_episodes))\n",
    "    for ep_i in episode_iter:\n",
    "        episode_iter.set_description(f\"Episode: {ep_i}\")\n",
    "\n",
    "        # Init S & chose A from S using policy derived from Q\n",
    "        state, _ = env.reset()\n",
    "        action = policy(state)\n",
    "\n",
    "        n_step_buffer = []\n",
    "        for t in range(max_ep_steps):\n",
    "            # Take action A, observe S' and R\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                if terminated:\n",
    "                    r = stats[\"ep_rewards\"][ep_i] + reward\n",
    "                    println(f\"EP:{ep_i} - Success! reward: {r}\")\n",
    "                break\n",
    "\n",
    "            # Chose A' from S' using policy derived from Q\n",
    "            next_action = policy(next_state)\n",
    "\n",
    "            if len(n_step_buffer) > n:\n",
    "                # Let the agent observe and make an update to the Q-function\n",
    "                observe(n_step_buffer)\n",
    "                n_step_buffer = []\n",
    "            else:\n",
    "                n_step_buffer.append((state, action, reward, next_state, next_action))\n",
    "\n",
    "            # Collect some stats\n",
    "            stats[\"ep_length\"][ep_i] = t\n",
    "            stats[\"ep_rewards\"][ep_i] += reward\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "    # Done!\n",
    "    env.close()\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "Q = Estimator()\n",
    "plot_q_surface(title=\"Initial Q-value function\")\n",
    "\n",
    "# 1-step SARSA\n",
    "learn(n=8)\n",
    "plot_q_surface(\"Final Q-value function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c188f6-e840-4a43-8793-39ccc51f9545",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0b9765e8-46d1-4174-976e-96816f60ce6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 -> Done! 199 steps. R:-200.0\n",
      "Episode: 1 -> Done! 199 steps. R:-200.0\n",
      "Episode: 2 -> Done! 176 steps. R:-177.0\n",
      "Episode: 3 -> Done! 145 steps. R:-146.0\n",
      "Episode: 4 -> Done! 145 steps. R:-146.0\n",
      "Episode: 5 -> Done! 150 steps. R:-151.0\n",
      "Episode: 6 -> Done! 143 steps. R:-144.0\n",
      "Episode: 7 -> Done! 141 steps. R:-142.0\n",
      "Episode: 8 -> Done! 140 steps. R:-141.0\n",
      "Episode: 9 -> Done! 170 steps. R:-171.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/code/scratch-RL/.venv/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:249: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gnwrapper\n",
    "from IPython import display\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
    "\n",
    "for ep_i in range(10):\n",
    "    ep_reward = 0\n",
    "    state, _ = env.reset()\n",
    "    for t in range(max_ep_steps):\n",
    "        action = policy(state)\n",
    "\n",
    "        # Take action A, observe S' and R\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        ep_reward += reward\n",
    "\n",
    "        # # render\n",
    "        # if t % 2 == 0:\n",
    "        #     plt.imshow(env.render())\n",
    "        #     display.display(plt.gcf())\n",
    "        #     display.clear_output(wait=True)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            print(f\"Episode: {ep_i} -> Done! {t} steps. R:{ep_reward}\")\n",
    "            break\n",
    "\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4445ba-b322-4c7c-ab97-e618fe94c5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
