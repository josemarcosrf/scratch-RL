{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2071cf5-21d4-428a-b6d5-965f2a153345",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "Also knwon as  `Monte Carlo Policy Gradient`, a **policy-gradient learning algorithm** with the following update rule:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t + \\alpha G_t \\frac{\\nabla_{\\theta}\\pi(A_t \\given S_t, \\theta_t)}{\\pi(A_t \\given S_t, \\theta_t)}\n",
    "$$\n",
    "\n",
    "Note that each increment is proportional to the product of a return $G_t$ and a vector; the gradient of the probability\n",
    "of taking the action taken divided by the probability of taking that action. The vector is the\n",
    "direction in parameter space that most increases the probability of repeating the action $A_t$ on future\n",
    "visits to state $S_t$. The update increases the parameter vector in this direction proportional to the\n",
    "return, and inversely proportional to the action probability. The former makes sense because it causes\n",
    "the parameter to move most in the directions that favor actions that yield the highest return. The latter\n",
    "makes sense because otherwise actions that are selected frequently are at an advantage.\n",
    "\n",
    "> Note that `REINFORCE` uses the complete return from time $t$, which includes all future rewards up until the end of the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63067be8-94ab-4d52-ac5b-fb4dd91a3b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
